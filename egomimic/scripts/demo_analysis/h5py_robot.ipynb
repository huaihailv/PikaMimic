{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import cv2\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "import json\n",
    "# from simarUtils import *\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "<HDF5 dataset \"train\": shape (50,), type \"|O\">\n",
      "<KeysViewHDF5 ['demo_0', 'demo_1', 'demo_10', 'demo_100', 'demo_101', 'demo_102', 'demo_103', 'demo_104', 'demo_105', 'demo_106', 'demo_107', 'demo_108', 'demo_109', 'demo_11', 'demo_110', 'demo_111', 'demo_112', 'demo_113', 'demo_114', 'demo_115', 'demo_116', 'demo_117', 'demo_118', 'demo_119', 'demo_12', 'demo_120', 'demo_121', 'demo_122', 'demo_123', 'demo_124', 'demo_125', 'demo_126', 'demo_127', 'demo_128', 'demo_129', 'demo_13', 'demo_130', 'demo_131', 'demo_132', 'demo_133', 'demo_134', 'demo_135', 'demo_136', 'demo_137', 'demo_138', 'demo_139', 'demo_14', 'demo_140', 'demo_141', 'demo_142', 'demo_143', 'demo_144', 'demo_145', 'demo_146', 'demo_15', 'demo_16', 'demo_17', 'demo_18', 'demo_19', 'demo_2', 'demo_20', 'demo_21', 'demo_22', 'demo_23', 'demo_24', 'demo_25', 'demo_26', 'demo_27', 'demo_28', 'demo_29', 'demo_3', 'demo_30', 'demo_31', 'demo_32', 'demo_33', 'demo_34', 'demo_35', 'demo_36', 'demo_37', 'demo_38', 'demo_39', 'demo_4', 'demo_40', 'demo_41', 'demo_42', 'demo_43', 'demo_44', 'demo_45', 'demo_46', 'demo_47', 'demo_48', 'demo_49', 'demo_5', 'demo_50', 'demo_51', 'demo_52', 'demo_53', 'demo_54', 'demo_55', 'demo_56', 'demo_57', 'demo_58', 'demo_59', 'demo_6', 'demo_60', 'demo_61', 'demo_62', 'demo_63', 'demo_64', 'demo_65', 'demo_66', 'demo_67', 'demo_68', 'demo_69', 'demo_7', 'demo_70', 'demo_71', 'demo_72', 'demo_73', 'demo_74', 'demo_75', 'demo_76', 'demo_77', 'demo_78', 'demo_79', 'demo_8', 'demo_80', 'demo_81', 'demo_82', 'demo_83', 'demo_84', 'demo_85', 'demo_86', 'demo_87', 'demo_88', 'demo_89', 'demo_9', 'demo_90', 'demo_91', 'demo_92', 'demo_93', 'demo_94', 'demo_95', 'demo_96', 'demo_97', 'demo_98', 'demo_99']>\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "81\n",
      "<HDF5 dataset \"actions_joints_act\": shape (81, 100, 14), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (81, 6), type \"<f8\">\n",
      "<HDF5 dataset \"joint_positions\": shape (81, 14), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (81, 480, 640, 3), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (90, 480, 640, 3), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/cobot_groceries.hdf5\", \"r\")\n",
    "print(len(data['data'].keys()))\n",
    "print(data['mask']['train'])\n",
    "print(data['data'].keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0'].attrs[\"num_samples\"])\n",
    "print(data['data']['demo_0']['actions_joints_act'])\n",
    "# print(data['data']['demo_0']['actions_xyz_act'][100])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_1']['obs']['front_img_1'])\n",
    "# print(data['data']['demo_0']['obs']['ee_pose'][100])\n",
    "# print(data['data']['demo_0']['obs']['joint_positions'][100])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1'][100])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "<HDF5 dataset \"train\": shape (100,), type \"|O\">\n",
      "<KeysViewHDF5 ['demo_0', 'demo_1', 'demo_10', 'demo_100', 'demo_101', 'demo_102', 'demo_103', 'demo_104', 'demo_105', 'demo_106', 'demo_107', 'demo_108', 'demo_109', 'demo_11', 'demo_110', 'demo_111', 'demo_112', 'demo_113', 'demo_114', 'demo_115', 'demo_116', 'demo_117', 'demo_118', 'demo_119', 'demo_12', 'demo_120', 'demo_121', 'demo_122', 'demo_123', 'demo_124', 'demo_125', 'demo_126', 'demo_127', 'demo_128', 'demo_129', 'demo_13', 'demo_130', 'demo_131', 'demo_132', 'demo_133', 'demo_134', 'demo_135', 'demo_136', 'demo_137', 'demo_138', 'demo_139', 'demo_14', 'demo_140', 'demo_141', 'demo_142', 'demo_143', 'demo_144', 'demo_145', 'demo_146', 'demo_147', 'demo_148', 'demo_149', 'demo_15', 'demo_150', 'demo_151', 'demo_152', 'demo_153', 'demo_154', 'demo_155', 'demo_156', 'demo_157', 'demo_158', 'demo_159', 'demo_16', 'demo_160', 'demo_161', 'demo_162', 'demo_163', 'demo_164', 'demo_165', 'demo_166', 'demo_167', 'demo_168', 'demo_169', 'demo_17', 'demo_170', 'demo_171', 'demo_172', 'demo_173', 'demo_174', 'demo_175', 'demo_176', 'demo_177', 'demo_178', 'demo_179', 'demo_18', 'demo_180', 'demo_181', 'demo_182', 'demo_183', 'demo_184', 'demo_185', 'demo_186', 'demo_187', 'demo_188', 'demo_189', 'demo_19', 'demo_190', 'demo_191', 'demo_192', 'demo_193', 'demo_194', 'demo_195', 'demo_196', 'demo_197', 'demo_198', 'demo_199', 'demo_2', 'demo_20', 'demo_200', 'demo_201', 'demo_202', 'demo_203', 'demo_204', 'demo_205', 'demo_206', 'demo_207', 'demo_208', 'demo_209', 'demo_21', 'demo_210', 'demo_211', 'demo_212', 'demo_213', 'demo_214', 'demo_215', 'demo_216', 'demo_217', 'demo_218', 'demo_219', 'demo_22', 'demo_220', 'demo_221', 'demo_222', 'demo_223', 'demo_224', 'demo_225', 'demo_226', 'demo_227', 'demo_228', 'demo_229', 'demo_23', 'demo_230', 'demo_231', 'demo_232', 'demo_233', 'demo_234', 'demo_235', 'demo_236', 'demo_237', 'demo_238', 'demo_239', 'demo_24', 'demo_240', 'demo_241', 'demo_242', 'demo_243', 'demo_244', 'demo_245', 'demo_246', 'demo_247', 'demo_248', 'demo_249', 'demo_25', 'demo_250', 'demo_251', 'demo_252', 'demo_253', 'demo_254', 'demo_255', 'demo_256', 'demo_257', 'demo_258', 'demo_259', 'demo_26', 'demo_260', 'demo_261', 'demo_262', 'demo_263', 'demo_264', 'demo_265', 'demo_266', 'demo_267', 'demo_268', 'demo_269', 'demo_27', 'demo_270', 'demo_271', 'demo_272', 'demo_273', 'demo_274', 'demo_275', 'demo_276', 'demo_277', 'demo_278', 'demo_279', 'demo_28', 'demo_280', 'demo_281', 'demo_282', 'demo_283', 'demo_284', 'demo_285', 'demo_286', 'demo_287', 'demo_288', 'demo_289', 'demo_29', 'demo_290', 'demo_291', 'demo_292', 'demo_293', 'demo_294', 'demo_295', 'demo_296', 'demo_297', 'demo_298', 'demo_299', 'demo_3', 'demo_30', 'demo_300', 'demo_31', 'demo_32', 'demo_33', 'demo_34', 'demo_35', 'demo_36', 'demo_37', 'demo_38', 'demo_39', 'demo_4', 'demo_40', 'demo_41', 'demo_42', 'demo_43', 'demo_44', 'demo_45', 'demo_46', 'demo_47', 'demo_48', 'demo_49', 'demo_5', 'demo_50', 'demo_51', 'demo_52', 'demo_53', 'demo_54', 'demo_55', 'demo_56', 'demo_57', 'demo_58', 'demo_59', 'demo_6', 'demo_60', 'demo_61', 'demo_62', 'demo_63', 'demo_64', 'demo_65', 'demo_66', 'demo_67', 'demo_68', 'demo_69', 'demo_7', 'demo_70', 'demo_71', 'demo_72', 'demo_73', 'demo_74', 'demo_75', 'demo_76', 'demo_77', 'demo_78', 'demo_79', 'demo_8', 'demo_80', 'demo_81', 'demo_82', 'demo_83', 'demo_84', 'demo_85', 'demo_86', 'demo_87', 'demo_88', 'demo_89', 'demo_9', 'demo_90', 'demo_91', 'demo_92', 'demo_93', 'demo_94', 'demo_95', 'demo_96', 'demo_97', 'demo_98', 'demo_99']>\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "205\n",
      "<HDF5 dataset \"actions_joints_act\": shape (205, 100, 12), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'joint_positions']>\n",
      "<HDF5 dataset \"ee_pose\": shape (205, 6), type \"<f8\">\n",
      "<HDF5 dataset \"joint_positions\": shape (205, 12), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (205, 480, 640, 3), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (205, 480, 640, 3), type \"<f8\">\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/human_groceries.hdf5\", \"r\")\n",
    "print(len(data['data'].keys()))\n",
    "print(data['mask']['train'])\n",
    "print(data['data'].keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0'].attrs[\"num_samples\"])\n",
    "print(data['data']['demo_0']['actions_joints_act'])\n",
    "# print(data['data']['demo_0']['actions_xyz_act'][100])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_1']['obs']['front_img_1'])\n",
    "\n",
    "# print(data['data']['demo_0']['obs']['ee_pose'][100])\n",
    "# print(data['data']['demo_0']['obs']['joint_positions'][100])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1'][100])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685\n",
      "1922\n",
      "1547\n",
      "1807\n",
      "1359\n",
      "1889\n",
      "1771\n",
      "2266\n",
      "1844\n",
      "1747\n",
      "1457\n",
      "1822\n",
      "1917\n",
      "1807\n",
      "1831\n",
      "1894\n",
      "1656\n",
      "1425\n",
      "1733\n",
      "1822\n",
      "2130\n",
      "1640\n",
      "1748\n",
      "1857\n",
      "1571\n",
      "1804\n",
      "1746\n",
      "1616\n",
      "1325\n",
      "1321\n",
      "1501\n",
      "1569\n",
      "1715\n",
      "1556\n",
      "1608\n",
      "1379\n",
      "1483\n",
      "1420\n",
      "1441\n",
      "1581\n",
      "1523\n",
      "1378\n",
      "1409\n",
      "1486\n",
      "3475\n",
      "1548\n",
      "1325\n",
      "1453\n",
      "1369\n",
      "1460\n",
      "1458\n",
      "1498\n",
      "1539\n",
      "1639\n",
      "1728\n",
      "1452\n",
      "1452\n",
      "1465\n",
      "1432\n",
      "1368\n",
      "1525\n",
      "1426\n",
      "1423\n",
      "1523\n",
      "1478\n",
      "1314\n",
      "1439\n",
      "1564\n",
      "1836\n",
      "1477\n",
      "1363\n",
      "1455\n",
      "1356\n",
      "1450\n",
      "1637\n",
      "1436\n",
      "1468\n",
      "1503\n",
      "1560\n",
      "1184\n",
      "1395\n",
      "1228\n",
      "1416\n",
      "1296\n",
      "1378\n",
      "1318\n",
      "1425\n",
      "1229\n",
      "1548\n",
      "1257\n",
      "1480\n",
      "1417\n",
      "1268\n",
      "1430\n",
      "1229\n",
      "1562\n",
      "1430\n",
      "1408\n",
      "1356\n",
      "1326\n",
      "3475\n",
      "1184\n",
      "44\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "len = 0\n",
    "min = 10000\n",
    "for i in range(100):\n",
    "    data = h5py.File(f\"/home/lvhuaihai/EgoMimic/datasets/cobot/data_pour3cups_cobot/episode_{i}.hdf5\", \"r\")\n",
    "    # print(len(data.keys()))\n",
    "    # print(data.keys())\n",
    "    print(data['observations']['images']['cam_high'].shape[0])\n",
    "    if data['observations']['images']['cam_high'].shape[0] > len:\n",
    "        len = data['observations']['images']['cam_high'].shape[0]\n",
    "        max_index = i\n",
    "    if data['observations']['images']['cam_high'].shape[0] < min:\n",
    "        min = data['observations']['images']['cam_high'].shape[0]\n",
    "        min_index = i\n",
    "    data.close()\n",
    "print(len)\n",
    "print(min)\n",
    "print(max_index)\n",
    "print(min_index)\n",
    "    # print(data['observations'].keys())\n",
    "    # print(data['observations']['images'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['eef_pose_L', 'eef_pose_R']>\n",
      "<KeysViewHDF5 ['gripper', 'space_angle', 'xyz']>\n",
      "<HDF5 dataset \"gripper\": shape (32036,), type \"<f2\">\n",
      "1.69\n",
      "<HDF5 dataset \"space_angle\": shape (3, 32036), type \"<f2\">\n",
      "<HDF5 dataset \"xyz\": shape (3, 32036), type \"<f2\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data = h5py.File(f\"/mnt/hpfs/baaiei/robot_data/pika/Basket_Folding/task_8-25-04-02/episode50/episode50.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['eef_pose_L'].keys())\n",
    "print(data['eef_pose_L/gripper'])\n",
    "print(data['eef_pose_L/gripper'][0])\n",
    "print(data['eef_pose_L/space_angle'])\n",
    "print(data['eef_pose_L/xyz'])\n",
    "# print(data['action'])\n",
    "# print(data['observations'].keys())\n",
    "# print(data['observations']['qpos'])\n",
    "# print(data['observations']['qpos'][0])\n",
    "# print(data['observations']['images'].keys())\n",
    "# print(data['observations']['images']['cam_high'].shape)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[387 387 387 387 387 387 387 387 387 387 387 387]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 387 387 387 387 387 387 387 387\n",
      " 387 387 387 387]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def resample_to_400_frames(data, target_length=400):\n",
    "        current_length = data\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            # 如果当前长度小于目标长度，重复最后一帧\n",
    "            len = target_length - current_length\n",
    "            padded_data = np.tile([current_length-1], len)\n",
    "            print(padded_data)\n",
    "            # padded_data = np.tile(last_frame, (target_length - current_length, 1) + (1,) * (data.ndim - 1))\n",
    "            return np.concatenate([np.linspace(0, current_length - 1, current_length, dtype=int), padded_data], axis=0)\n",
    "        \n",
    "        # 计算均匀抽帧的间隔\n",
    "        indices = np.linspace(0, current_length - 1, target_length, dtype=int)\n",
    "        \n",
    "        return indices\n",
    "\n",
    "print(resample_to_400_frames(388))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Hand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"train\": shape (100,), type \"|O\">\n",
      "100\n",
      "<KeysViewHDF5 ['demo_0', 'demo_1', 'demo_10', 'demo_11', 'demo_12', 'demo_13', 'demo_14', 'demo_15', 'demo_16', 'demo_17', 'demo_18', 'demo_19', 'demo_2', 'demo_20', 'demo_21', 'demo_22', 'demo_23', 'demo_24', 'demo_25', 'demo_26', 'demo_27', 'demo_28', 'demo_29', 'demo_3', 'demo_30', 'demo_31', 'demo_32', 'demo_33', 'demo_34', 'demo_35', 'demo_36', 'demo_37', 'demo_38', 'demo_39', 'demo_4', 'demo_40', 'demo_41', 'demo_42', 'demo_43', 'demo_44', 'demo_45', 'demo_46', 'demo_47', 'demo_48', 'demo_49', 'demo_5', 'demo_50', 'demo_51', 'demo_52', 'demo_53', 'demo_54', 'demo_55', 'demo_56', 'demo_57', 'demo_58', 'demo_59', 'demo_6', 'demo_60', 'demo_61', 'demo_62', 'demo_63', 'demo_64', 'demo_65', 'demo_66', 'demo_67', 'demo_68', 'demo_69', 'demo_7', 'demo_70', 'demo_71', 'demo_72', 'demo_73', 'demo_74', 'demo_75', 'demo_76', 'demo_77', 'demo_78', 'demo_79', 'demo_8', 'demo_80', 'demo_81', 'demo_82', 'demo_83', 'demo_84', 'demo_85', 'demo_86', 'demo_87', 'demo_88', 'demo_89', 'demo_9', 'demo_90', 'demo_91', 'demo_92', 'demo_93', 'demo_94', 'demo_95', 'demo_96', 'demo_97', 'demo_98', 'demo_99']>\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "100\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (100, 25, 6), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (100, 6), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (100, 480, 640, 3), type \"<f8\">\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "100\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (100, 25, 6), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (100, 6), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (100, 480, 640, 3), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/cobot_groceries_100pairs_25steps.hdf5\", \"r\")\n",
    "print(data['mask']['train'])\n",
    "print(len(data['data'].keys()))\n",
    "print(data['data'].keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0'].attrs[\"num_samples\"])\n",
    "# print(data['data']['demo_0']['actions_xyz'])\n",
    "print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_mask'])\n",
    "print(data['data']['demo_1'].keys())\n",
    "print(data['data']['demo_1'].attrs[\"num_samples\"])\n",
    "# print(data['data']['demo_1']['actions_xyz'])\n",
    "print(data['data']['demo_1']['actions_xyz_act'])\n",
    "print(data['data']['demo_1']['obs'].keys())\n",
    "print(data['data']['demo_1']['obs']['ee_pose'])\n",
    "print(data['data']['demo_1']['obs']['front_img_1'])\n",
    "# print(data['data']['demo_1']['obs']['front_img_1_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"train\": shape (1,), type \"|O\">\n",
      "<HDF5 dataset \"valid\": shape (1,), type \"|O\">\n",
      "1\n",
      "<KeysViewHDF5 ['demo_0']>\n",
      "<KeysViewHDF5 ['actions_joints', 'actions_joints_act', 'actions_xyz', 'actions_xyz_act', 'obs']>\n",
      "5000\n",
      "<HDF5 dataset \"actions_xyz\": shape (5000, 10, 6), type \"<f8\">\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (5000, 100, 6), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'front_img_1_line', 'front_img_1_masked', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (5000, 6), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (5000, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"front_img_1_line\": shape (5000, 480, 640, 3), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/groceries_robot.hdf5\", \"r\")\n",
    "print(data['mask']['train'])\n",
    "print(data['mask']['valid'])\n",
    "print(len(data['data'].keys()))\n",
    "print(data['data'].keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0'].attrs[\"num_samples\"])\n",
    "print(data['data']['demo_0']['actions_xyz'])\n",
    "print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1_line'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'demo_1'\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "<HDF5 dataset \"actions_joints_act\": shape (3000, 100, 7), type \"<f8\">\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (3000, 100, 3), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'front_img_1_line', 'front_img_1_masked', 'joint_positions', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (3000, 3), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (3000, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"front_img_1_line\": shape (3000, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"joint_positions\": shape (3000, 7), type \"<f4\">\n",
      "<HDF5 dataset \"right_wrist_img\": shape (3000, 480, 640, 3), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/bowlplace_robot.hdf5\", \"r\")\n",
    "print(data['mask']['train'][1])\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0']['actions_joints_act'])\n",
    "print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_mask'])\n",
    "print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "print(data['data']['demo_0']['obs']['right_wrist_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data', 'mask']>\n",
      "<KeysViewHDF5 ['train', 'valid']>\n",
      "b'demo_1'\n",
      "<KeysViewHDF5 ['actions_joints', 'actions_joints_act', 'actions_xyz', 'actions_xyz_act', 'obs']>\n",
      "<KeysViewHDF5 ['actions_joints', 'actions_joints_act', 'actions_xyz', 'actions_xyz_act', 'obs']>\n",
      "<HDF5 dataset \"actions_joints_act\": shape (5000, 100, 14), type \"<f8\">\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (5000, 100, 6), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'front_img_1_line', 'front_img_1_masked', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"ee_pose\": shape (5000, 6), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (5000, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"front_img_1_line\": shape (5000, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"joint_positions\": shape (5000, 14), type \"<f4\">\n",
      "<HDF5 dataset \"right_wrist_img\": shape (5000, 480, 640, 3), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/smallclothfold_robot.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['mask'].keys())\n",
    "print(data['mask']['train'][0])\n",
    "\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_1'].keys())\n",
    "print(data['data']['demo_0']['actions_joints_act'])\n",
    "print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['dem2_0']['obs']['front_img_1_mask'])\n",
    "print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "print(data['data']['demo_0']['obs']['right_wrist_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data', 'mask']>\n",
      "<KeysViewHDF5 ['demo_0', 'demo_1', 'demo_10', 'demo_100', 'demo_101', 'demo_102', 'demo_103', 'demo_104', 'demo_105', 'demo_106', 'demo_107', 'demo_108', 'demo_109', 'demo_11', 'demo_110', 'demo_111', 'demo_112', 'demo_113', 'demo_114', 'demo_115', 'demo_116', 'demo_117', 'demo_118', 'demo_119', 'demo_12', 'demo_120', 'demo_121', 'demo_122', 'demo_123', 'demo_124', 'demo_125', 'demo_126', 'demo_127', 'demo_128', 'demo_129', 'demo_13', 'demo_130', 'demo_131', 'demo_132', 'demo_133', 'demo_134', 'demo_135', 'demo_136', 'demo_137', 'demo_138', 'demo_139', 'demo_14', 'demo_140', 'demo_141', 'demo_142']>\n",
      "<KeysViewHDF5 ['actions_xyz', 'actions_xyz_act', 'obs']>\n",
      "<HDF5 dataset \"actions_xyz_act\": shape (300, 100, 6), type \"<f8\">\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'front_img_1_line', 'front_img_1_mask', 'front_img_1_masked']>\n",
      "<HDF5 dataset \"ee_pose\": shape (300, 6), type \"<f8\">\n",
      "<HDF5 dataset \"front_img_1\": shape (300, 480, 640, 3), type \"|u1\">\n",
      "<HDF5 dataset \"front_img_1_line\": shape (300, 480, 640, 3), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/groceries_human.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['data'].keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "# print(data['data']['demo_0']['actions_joints_act'])\n",
    "print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# print(data['data']['dem2_0']['obs']['front_img_1_mask'])\n",
    "# print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "# print(data['data']['demo_0']['obs']['right_wrist_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['action', 'observations']>\n",
      "<HDF5 dataset \"action\": shape (330, 128), type \"<f4\">\n",
      "<KeysViewHDF5 ['images', 'qpos']>\n",
      "<KeysViewHDF5 ['cam_high', 'cam_left_wrist', 'cam_right_wrist']>\n",
      "(59606,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51218,)\n",
      "(40049,)\n",
      "<HDF5 dataset \"qpos\": shape (330, 128), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/realbench/Dataset/fold_the_adult_shorts_neatly/sample/episode_80.hdf5\", \"r\")\n",
    "# print(len(data['data'].keys()))\n",
    "print(data.keys())\n",
    "print(data['action'])\n",
    "print(data['observations'].keys())\n",
    "print(data['observations']['images'].keys())\n",
    "print(data['observations']['images']['cam_high'][0].shape)\n",
    "print(data['observations']['images']['cam_left_wrist'][0].shape)\n",
    "print(data['observations']['images']['cam_right_wrist'][0].shape)\n",
    "print(data['observations']['qpos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['action', 'observations']>\n",
      "<KeysViewHDF5 ['images', 'qpos']>\n",
      "<HDF5 dataset \"qpos\": shape (80, 128), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/data_cobot_80/episode_143_80.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['observations'].keys())\n",
    "print(data['observations']['qpos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/lvhuaihai/EgoMimic/datasets/ego.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/ego.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['data'].keys())\n",
    "print(data['mask'].keys())\n",
    "print(data['mask']['train'])\n",
    "print(data['data']['demo_0'].keys())\n",
    "# print(data['data']['demo_0']['actions_joints_act'])\n",
    "# print(data['data']['demo_0']['actions_xyz_act'])\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "# print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# # print(data['data']['dem2_0']['obs']['front_img_1_mask'])\n",
    "\n",
    "# print(data['data']['demo_94']['obs']['joint_positions'])\n",
    "# print(data['data']['demo_94'].keys())\n",
    "# print(data['data']['demo_94']['obs'].keys())\n",
    "data.close()\n",
    "# print(data['data']['demo_0']['obs']['right_wrist_img'])\n",
    "# print(data['data']['demo_0'].keys())\n",
    "# print(data['data']['demo_0']['actions_joints_act'])\n",
    "# print(data['data']['demo_0']['actions_xyz_act'])\n",
    "# print(data['data']['demo_0']['obs'].keys())\n",
    "# print(data['data']['demo_0']['obs']['ee_pose'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "# print(data['data']['demo_0']['obs']['front_img_1_line'])\n",
    "# # print(data['data']['dem2_0']['obs']['front_img_1_mask'])\n",
    "# print(data['data']['demo_0']['obs']['joint_positions'])\n",
    "# print(data['data']['demo_0']['obs']['right_wrist_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data', 'mask']>\n",
      "<KeysViewHDF5 ['actions_joints_act', 'actions_xyz_act', 'obs']>\n",
      "<KeysViewHDF5 ['ee_pose', 'front_img_1', 'joint_positions', 'left_wrist_img', 'right_wrist_img']>\n",
      "<HDF5 dataset \"front_img_1\": shape (392, 480, 640, 3), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data = h5py.File(\"/home/lvhuaihai/EgoMimic/datasets/aloha_fruit.hdf5\", \"r\")\n",
    "print(data.keys())\n",
    "print(data['data']['demo_0'].keys())\n",
    "print(data['data']['demo_0']['obs'].keys())\n",
    "print(data['data']['demo_0']['obs']['front_img_1'])\n",
    "\n",
    "# print(data['action'][0])\n",
    "# print(data['action'][50])\n",
    "# print(data['action'][100])\n",
    "# print(data['action'][150])\n",
    "# print(data['action'][200])\n",
    "# print(data['action'][240:,-1])\n",
    "# print(data['observations'].keys())\n",
    "# # print(data['observations']['effort'])\n",
    "# # print(data['observations']['qpos'])\n",
    "# # print(data['observations']['qvel'])\n",
    "# print(data['observations']['images'].keys())\n",
    "# print(data['observations']['images']['cam_high'][0])\n",
    "# print(data['observations']['images']['cam_left_wrist'][491])\n",
    "# # print(data['observations']['images']['cam_right_wrist'])\n",
    "# print(data['base_action'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
